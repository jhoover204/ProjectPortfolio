{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOyOkJJA7Foy"
   },
   "source": [
    "# Machine Learning in Python - Project 1\n",
    "\n",
    "Due Friday, March 11th by 5 pm.\n",
    "\n",
    "*Anoushka Ghosh, Keith Tung, Jonathan Hoover, Rebekah Kiner*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CS_CdPvv1YQq",
    "outputId": "83fd3feb-112a-4804-f58b-a56375c0c16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "project1.ipynb\tproject1.zip  proj-material\n"
     ]
    }
   ],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pukkDHxI0vs-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: schrutepy in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: ipython>=6 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from schrutepy) (8.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from schrutepy) (1.4.1)\n",
      "Requirement already satisfied: twine>=3 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from schrutepy) (3.8.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from schrutepy) (0.37.1)\n",
      "Requirement already satisfied: pip in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from schrutepy) (21.2.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (5.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (58.0.4)\n",
      "Requirement already satisfied: pygments in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (3.0.28)\n",
      "Requirement already satisfied: black in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (22.1.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (0.7.5)\n",
      "Requirement already satisfied: stack-data in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (0.1.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from ipython>=6->schrutepy) (0.18.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from jedi>=0.16->ipython>=6->schrutepy) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6->schrutepy) (0.2.5)\n",
      "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (0.9.1)\n",
      "Requirement already satisfied: rfc3986>=1.4.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (2.0.0)\n",
      "Requirement already satisfied: pkginfo>=1.8.1 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (1.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (4.11.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.14 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (4.62.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (1.26.8)\n",
      "Requirement already satisfied: readme-renderer>=21.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (32.0)\n",
      "Requirement already satisfied: keyring>=15.1 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from twine>=3->schrutepy) (23.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from importlib-metadata>=3.6->twine>=3->schrutepy) (3.7.0)\n",
      "Requirement already satisfied: pywin32-ctypes!=0.1.0,!=0.1.1 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from keyring>=15.1->twine>=3->schrutepy) (0.2.0)\n",
      "Requirement already satisfied: bleach>=2.1.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from readme-renderer>=21.0->twine>=3->schrutepy) (4.1.0)\n",
      "Requirement already satisfied: docutils>=0.13.1 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from readme-renderer>=21.0->twine>=3->schrutepy) (0.18.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from bleach>=2.1.0->readme-renderer>=21.0->twine>=3->schrutepy) (21.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from bleach>=2.1.0->readme-renderer>=21.0->twine>=3->schrutepy) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from bleach>=2.1.0->readme-renderer>=21.0->twine>=3->schrutepy) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from requests>=2.20->twine>=3->schrutepy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from requests>=2.20->twine>=3->schrutepy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from requests>=2.20->twine>=3->schrutepy) (3.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from black->ipython>=6->schrutepy) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from black->ipython>=6->schrutepy) (4.1.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from black->ipython>=6->schrutepy) (2.0.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from black->ipython>=6->schrutepy) (8.0.4)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from black->ipython>=6->schrutepy) (2.5.1)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from black->ipython>=6->schrutepy) (0.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from packaging->bleach>=2.1.0->readme-renderer>=21.0->twine>=3->schrutepy) (3.0.7)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from pandas->schrutepy) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from pandas->schrutepy) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from pandas->schrutepy) (2021.3)\n",
      "Requirement already satisfied: executing in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from stack-data->ipython>=6->schrutepy) (0.8.2)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from stack-data->ipython>=6->schrutepy) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\jonathan\\anaconda3\\envs\\mlp-proj1\\lib\\site-packages (from stack-data->ipython>=6->schrutepy) (2.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install schrutepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqPpPRE37Fo0"
   },
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb4qh-pi7Fo1"
   },
   "outputs": [],
   "source": [
    "#%%\n",
    "#os library version\n",
    "from pathlib import Path\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "# from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "# extra modules\n",
    "from schrutepy import schrutepy\n",
    "\n",
    "##Global variables\n",
    "##Threshold total line percentage for being a \"main character\"\n",
    "P = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjs5785u7Fo1"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "dir = os.getcwd()\n",
    "file = dir + \"/the_office.csv\"\n",
    "d = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jRx56GUG4xxM",
    "outputId": "2f846c2b-8c7f-4f23-8edc-e91d5d7d37a0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Colab Notebooks/mlp/project_1/proj-material'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTRJ2eb740dg"
   },
   "outputs": [],
   "source": [
    "transcripts = schrutepy.load_schrute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K314dGEL7Fo1"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "*This section should include a brief introduction to the task and the data (assume this is a report you are delivering to a client). If you use any additional data sources, you should introduce them here and discuss why they were included.*\n",
    "\n",
    "*Briefly outline the approaches being used and the conclusions that you are able to draw.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P6Vdzbo7Fo2"
   },
   "source": [
    "## 2. Exploratory Data Analysis and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arg9_dYE7Fo2"
   },
   "source": [
    "*Include a detailed discussion of the data with a particular emphasis on the features of the data that are relevant for the subsequent modeling. Including visualizations of the data is strongly encouraged - all code and plots must also be described in the write up. Think carefully about whether each plot needs to be included in your final draft - your report should include figures but they should be as focused and impactful as possible.*\n",
    "\n",
    "*Additionally, this section should also implement and describe any preprocessing / feature engineering of the data. Specifically, this should be any code that you use to generate new columns in the data frame `d`. All of this processing is explicitly meant to occur before we split the data in to training and testing subsets. Processing that will be performed as part of an sklearn pipeline can be mentioned here but should be implemented in the following section.*\n",
    "\n",
    "*All code and figures should be accompanied by text that provides an overview / context to what is being done or presented.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vu65-KLuANKK"
   },
   "outputs": [],
   "source": [
    "##copy original data so we don't lose it\n",
    "df = d.copy()\n",
    "\n",
    "##Note that some directors are repeated with different spellings. \n",
    "##Need to fix before any future data solutions\n",
    "wrongdir_dict = {\"Greg Daneils\": \"Greg Daniels\", \"Charles McDougal\": \"Charles McDougall\",\n",
    "                 \"Claire Scanlong\":\"Claire Scanlon\"}\n",
    "df[\"director\"].replace(wrongdir_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3EBHOA8AQ07"
   },
   "outputs": [],
   "source": [
    "##combining transcript data with original data to get character line information/important characters\n",
    "\n",
    "##string cols\n",
    "str_cols = (transcripts.applymap(type) == str).all(0)\n",
    "\n",
    "##Lower names, character's and lines for easier processing\n",
    "transcripts = transcripts.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "##Remove spaces in character (some are incorrectly written like \"jim \")\n",
    "transcripts['character'] = transcripts['character'].str.replace(\" \", \"\")\n",
    "transcripts['season_ep'] = transcripts['season'].astype(str) + \"_\" + transcripts['episode'].astype(str)\n",
    "\n",
    "# ##Per Episode Character lines\n",
    "# transcripts.character.unique()\n",
    "line_sum = transcripts.groupby([\"season_ep\", \"character\"]).size().reset_index(name = \"lines\") ##per character and episode lines\n",
    "line_sum = line_sum.sort_values(ascending = False, by = \"lines\")##sort by lines\n",
    "\n",
    "##Total line percentage per character\n",
    "line_perc = line_sum.groupby(\"character\").agg({\"lines\": \"sum\"}).sort_values(ascending = False, by = \"lines\")\n",
    "line_perc['percentage'] = line_perc/(line_perc.sum())\n",
    "\n",
    "##Greater than 1% line share\n",
    "main_char = line_perc.loc[line_perc['percentage'] > P].index\n",
    "\n",
    "##Top 10 line speakers (main_char)\n",
    "# main_char = (line_perc.nlargest(10, 'percentage')).index\n",
    "\n",
    "##per episode lines for main characters\n",
    "line_main = line_sum.loc[line_sum['character'].isin(main_char)]\n",
    "\n",
    "##Join main data with lines per character data\n",
    "\n",
    "##convert line data into dataframe with each row as a season/episode and each column as the main character lines\n",
    "char_lines = line_main.pivot_table(values='lines', index='season_ep', columns='character').reset_index()\n",
    "char_lines = char_lines.fillna(0) ##fill NA values with zero because they are only not present if no lines were spoken\n",
    "\n",
    "##calculate character line percentage per episode\n",
    "perc_cols = char_lines.select_dtypes(include=np.number).columns + \"_perc\" ##create column names\n",
    "char_lines[perc_cols] = char_lines.select_dtypes(include=np.number).div(d.n_lines, axis = 0) ##per row char line percentages\n",
    "\n",
    "##Create character dummy variables for if present\n",
    "main_char_dummy = [str(x) + \"_dummy\" for x in main_char]\n",
    "char_lines[main_char_dummy] = (char_lines[main_char] > 0).astype(int)\n",
    "\n",
    "##copy original data so we don't lose it\n",
    "df = d.copy()\n",
    "\n",
    "\n",
    "##Note that some directors are repeated with different spellings. \n",
    "##Need to fix before any future data solutions\n",
    "wrongdir_dict = {\"Greg Daneils\": \"Greg Daniels\", \"Charles McDougal\": \"Charles McDougall\",\n",
    "                 \"Claire Scanlong\":\"Claire Scanlon\"}\n",
    "df[\"director\"].replace(wrongdir_dict, inplace=True)\n",
    "\n",
    "##create season_ep column for d to join by\n",
    "df['season_ep'] = df['season'].astype(str) + \"_\" + df['episode'].astype(str)\n",
    "\n",
    "##join the main data with the lines per character data\n",
    "df = pd.merge(df, char_lines, on = 'season_ep', how = 'left')\n",
    "\n",
    "\n",
    "##Want to get a list of all writers and a list of all directors \n",
    "##Create new columns where the presence of each writer is it's own variable (one-hot encoding)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['writer', 'director']) ##contains multiple director combinations\n",
    "\n",
    "\n",
    "# convert string to lists\n",
    "#df[[\"year\", \"month\", \"day\"]] = pd.DataFrame(d.air_date.str.split(\"-\").tolist()).astype(int)\n",
    "# df[\"writer\"]                 = df[\"writer\"].str.split(\";\")\n",
    "# df[\"main_chars\"]             = df[\"main_chars\"].str.split(\";\")\n",
    "# df[\"director\"]               = df[\"director\"].str.split(\";\")\n",
    "\n",
    "# # Multiple OHE for columns with arrays\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# writers_df = pd.DataFrame(mlb.fit_transform(df[\"director\"]), columns=mlb.classes_+ \"_dummy\")\n",
    "# directors_df = pd.DataFrame(mlb.fit_transform(df[\"writer\"]), columns=mlb.classes_+ \"_dummy\")\n",
    "# # adat = pd.DataFrame(mlb.fit_transform(df.iloc[\"main_chars\"]), columns=mlb.classes_+ \"_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SXfnzSnAQ-B"
   },
   "outputs": [],
   "source": [
    "##We want to drop some of the writers and directors if they don't appear often\n",
    "\n",
    "\n",
    "##################################################################################################################\n",
    "##Start of Functions##\n",
    "\n",
    "##Function to group \"low\" appearance directors/writers into one \"low-appearance\" column dummy variable\n",
    "##Removes the original columns for those low-appearane writers directors\n",
    "def categorize_low(column_type, df, threshold):\n",
    "    ##if writer/director shows up less than or equal to the threshold, re-categorize to \"Low-Appearance\"\n",
    "    items_eps = df.sum(axis = 0)\n",
    "    low_appearance = list(items_eps[items_eps <= threshold].index) \n",
    "    new_col = \"low_appearance\" + \"_\" + column_type\n",
    "\n",
    "    ##create new writer column for low-appearance\n",
    "    df_new = df.copy()\n",
    "    df_new[new_col] = df_new[low_appearance].sum(axis = 1) > 0 ##Identifies episodes where a low_appearance writer/dir is\n",
    "    df_new[new_col] = df_new[new_col].astype(int) ##convert boolean to integer\n",
    "    \n",
    "    ##drop original low_appearance columns\n",
    "    df_new.drop(columns = low_appearance, inplace = True)\n",
    "    \n",
    "    return(df_new)\n",
    "\n",
    "\n",
    "##End of Functions##\n",
    "##################################################################################################################\n",
    "\n",
    "\n",
    "##group low-appearance writers and remove columns for those low-appearance writers/directors\n",
    "writers_df = df.filter(regex = \"writer_\") ##get dummy writer data\n",
    "writers_df_cols = writers_df.columns ##get column names for dummy writer data\n",
    "writers_df = categorize_low(column_type = \"writer\", df = writers_df, threshold = 2) ##combine low-appearance into one col\n",
    "\n",
    "##group low-appearance directors and remove columns for those low-appearance writers/directors\n",
    "directors_df = df.filter(regex = \"director_\") ##get dummy director data\n",
    "directors_df_cols = directors_df.columns ##get column names for dummy directors\n",
    "directors_df = categorize_low(column_type = \"director\", df = directors_df, threshold = 2) ##combine low appearance into col\n",
    "\n",
    "\n",
    "##Add the writer and director data to overall data\n",
    "writer_director_col = list(writers_df_cols) + list(directors_df_cols)\n",
    "df = pd.concat([df.drop(columns = writer_director_col), writers_df, directors_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQdZH5RbARE7"
   },
   "outputs": [],
   "source": [
    "##Create multipart episode columns\n",
    "\n",
    "# Create column to indicate if said episode consist of multi parts\n",
    "p = re.compile(\"Parts 1&2\")\n",
    "df[\"multi_part\"] = [int(not int(pd.isnull(re.search(p,i)))) for i in df[\"episode_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHcjfKlEARLz"
   },
   "outputs": [],
   "source": [
    "##Replace any spaces in column names with \"_\"\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "# remove unused columns and observations\n",
    "col_drop = [\"season\",\"episode_name\", \"total_votes\", \"season_ep\", \"air_date\", \"episode\", \"main_chars\"]\n",
    "p        = re.compile(\"Part [12]\")\n",
    "row_drop = [pd.isnull(re.search(p,i)) for i in df[\"episode_name\"]]\n",
    "\n",
    "##Create filtered data with columns dropped\n",
    "fdat     = df.drop(col_drop,axis=1).iloc[row_drop,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xCrN95zARRg"
   },
   "outputs": [],
   "source": [
    "##save raw and filter data\n",
    "\n",
    "##create data directory if it doesn't exist\n",
    "if not os.path.exists(\"./data\"): os.mkdir(\"./data\")\n",
    "\n",
    "# %%\n",
    "df.to_csv(\"data/full_raw_dat.csv\", index=False)\n",
    "fdat.to_csv(\"data/full_filtered_dat.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBIXeHvRAix6"
   },
   "source": [
    "**Standardizing and Splitting Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "g3yFDmQAARXt",
    "outputId": "0fca7a8f-0527-4b89-fcbf-775b67a20d2c",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-716686266913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# read full data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_filtered_dat.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# separate response and explanatory data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'full_filtered_dat.csv'"
     ]
    }
   ],
   "source": [
    "SEED = 55 ##for reproducibility\n",
    "\n",
    "#%%\n",
    "# read full data\n",
    "fdat = pd.read_csv(\"full_filtered_dat.csv\")\n",
    "\n",
    "# separate response and explanatory data\n",
    "X = fdat.drop(\"imdb_rating\", axis=1)\n",
    "y = fdat.imdb_rating\n",
    "\n",
    "##Creating categorical version of response data for potential classifiers instead of regressors\n",
    "cuts = [6, 7.5, 8, 8.5, 9, 10] ##does not include the lowest; so here would make 4 bins between each of the numbers\n",
    "binnames = ['5','4', '3', '2', '1'] ##name of bins with 1 being the best rating\n",
    "y_class = pd.cut(y, cuts, labels = binnames)\n",
    "\n",
    "\n",
    "# binlist = [y <= 7.5, y <= 8.5, y <= 9, y <= 10] ##cutoffs for different imdb categories\n",
    "# binnames = ['1', '2', '3', \"4\"] ##names for the categories\n",
    "# y_class = pd.Series(np.select(binlist, binnames, default='unknown')) \n",
    "\n",
    "\n",
    "# np.unique(y_class, return_counts=True)\n",
    "\n",
    "# write raw files\n",
    "\n",
    "##make new file\n",
    "Path(f\"./data/seed_{SEED}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "##split data into train test\n",
    "##Numerical data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED, stratify = y_class)\n",
    "\n",
    "##binned data (X_test and X_train are the same as above since we use the same seed, so no need to respecify)\n",
    "##Unpack to _ and _ for both since we made above\n",
    "_, _, y_train_cl, y_test_cl = train_test_split(X, y_class, test_size=0.33, random_state=SEED, stratify = y_class)\n",
    "\n",
    "\n",
    "\n",
    "##Write to csvs for later\n",
    "\n",
    "##numerical data\n",
    "X_train.to_csv(dir + f\"/data/SEED_{SEED}\", index=False)\n",
    "X_test.to_csv(f\"data/SEED_{SEED}/X_test_raw.csv\", index=False)\n",
    "y_train.to_csv(f\"data/SEED_{SEED}/y_train.csv\", index=False)\n",
    "y_test.to_csv(f\"data/SEED_{SEED}/y_test.csv\", index=False)\n",
    "\n",
    "##binned data\n",
    "y_train_cl.to_csv(f\"data/SEED_{SEED}/y_train_cl.csv\", index=False)\n",
    "y_test_cl.to_csv(f\"data/SEED_{SEED}/y_test_cl.csv\", index=False)\n",
    "\n",
    "# create pipeline for scalers\n",
    "std_scale = Pipeline([('standard', StandardScaler())])\n",
    "minmax_scale = Pipeline([('minmax', MinMaxScaler())])\n",
    "\n",
    "# select columns that require scaling\n",
    "##select dummy variables (All dummies are numeric, but binary)\n",
    "##Therefore we select from numerics, but only if labeled with dummy writer or director\n",
    "\n",
    "scale_col = X_train.select_dtypes(include = np.number) ##subset the numeric columns\n",
    "cat_cols = scale_col.filter(regex = 'dummy|writer|director').columns ##subset the columns with dummy/writer/director\n",
    "\n",
    "scale_col = [col for col in scale_col if col not in cat_cols] ##update scale_col with numeric columns not in dummy vars\n",
    "\n",
    "# scale_col = X_train.iloc[:,0:24].select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "\n",
    "##Define column transformers for scaling and min/max transformations\n",
    "minmax_prep = ColumnTransformer(\n",
    "        remainder='passthrough', ##keep all columns not specified in transformer\n",
    "        transformers=[\n",
    "            ('minmax', minmax_scale , scale_col), ##apply minmax_scale to scale_col\n",
    "        ])\n",
    "\n",
    "std_prep = ColumnTransformer(\n",
    "        remainder='passthrough', ##keep all columns not specified in transformer\n",
    "        transformers=[\n",
    "            ('std', std_scale , scale_col), ##apply std_scale to scale_col\n",
    "        ])\n",
    "\n",
    "\n",
    "#%%\n",
    "# fit minmax on training data\n",
    "minmax_prep.fit(X_train)\n",
    "minmax_train = pd.DataFrame(minmax_prep.transform(X_train), columns = X_train.columns)\n",
    "minmax_test = pd.DataFrame(minmax_prep.transform(X_test), columns = X_train.columns)##Transform test with fit from training\n",
    "\n",
    "##fit scaler on training data\n",
    "std_prep.fit(X_train)\n",
    "std_train = pd.DataFrame(std_prep.transform(X_train), columns = X_train.columns)\n",
    "std_test = pd.DataFrame(std_prep.transform(X_test), columns = X_train.columns) ##Transform test data with fit from training \n",
    "\n",
    "##write to csv\n",
    "std_train.to_csv(f\"data/SEED_{SEED}/X_train_std.csv\", index=False)\n",
    "std_test.to_csv(f\"data/SEED_{SEED}/X_test_std.csv\", index=False)\n",
    "minmax_train.to_csv(f\"data/SEED_{SEED}/X_train_minmax.csv\", index=False)\n",
    "minmax_test.to_csv(f\"data/SEED_{SEED}/X_test_minmax.csv\", index=False)\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ad7J-Yw67Fo3"
   },
   "source": [
    "## 3. Model Fitting and Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXdvOaZs7Fo3"
   },
   "source": [
    "*In this section you should detail your choice of model and describe the process used to refine and fit that model. You are strongly encouraged to explore many different modeling methods (e.g. linear regression, regression trees, lasso, etc.) but you should not include a detailed narrative of all of these attempts. At most this section should mention the methods explored and why they were rejected - most of your effort should go into describing the model you are using and your process for tuning and validatin it.*\n",
    "\n",
    "*For example if you considered a linear regression model, a classification tree, and a lasso model and ultimately settled on the linear regression approach then you should mention that other two approaches were tried but do not include any of the code or any in depth discussion of these models beyond why they were rejected. This section should then detail is the development of the linear regression model in terms of features used, interactions considered, and any additional tuning and validation which ultimately led to your final model.* \n",
    "\n",
    "*This section should also include the full implementation of your final model, including all necessary validation. As with figures, any included code must also be addressed in the text of the document.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edQVULLU7Fo3"
   },
   "source": [
    "## 4. Discussion & Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fi0yKPrQ7Fo4"
   },
   "source": [
    "*In this section you should provide a general overview of your final model, its performance, and reliability. You should discuss what the implications of your model are in terms of the included features, predictive performance, and anything else you think is relevant.*\n",
    "\n",
    "*This should be written with a target audience of a NBC Universal executive who is with the show and  university level mathematics but not necessarily someone who has taken a postgraduate statistical modeling course. Your goal should be to convince this audience that your model is both accurate and useful.*\n",
    "\n",
    "*Finally, you should include concrete recommendations on what NBC Universal should do to make their reunion episode a popular as possible.*\n",
    "\n",
    "*Keep in mind that a negative result, i.e. a model that does not work well predictively, that is well explained and justified in terms of why it failed will likely receive higher marks than a model with strong predictive performance but with poor or incorrect explinations / justifications.*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
